{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "def80828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries and packages\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "# Creating list to append tweet data \n",
    "tweets_list_22 = []\n",
    "tweets_list_21 = []\n",
    "tweets_list_20 = []\n",
    "tweets_list_19 = []\n",
    "\n",
    "#Setting search words that relates to tweets about veganism \n",
    "key= ['#vegan', '#meatreduction','#plant based']\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for keyword in key:\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper\n",
    "                             (f'{keyword} lang:en since:2022-01-01 until:2022-07-20')\n",
    "                             .get_items()): #declaring the search word, language of tweet and \n",
    "                                            #the year 2022 for which tweets are extracted\n",
    "        if i>15000: #setting number of tweets to scrape\n",
    "            break\n",
    "        tweets_list_22.append([tweet.date, tweet.id, \n",
    "                             tweet.content, tweet.user.username, \n",
    "                             tweet.coordinates]) #declare the attributes to be returned\n",
    "        \n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper\n",
    "                             (f'{keyword} lang:en since:2021-01-01 until:2021-12-31')\n",
    "                             .get_items()): #declaring the search word, language of tweet and \n",
    "                                            #the year 2021 for which tweets are extracted\n",
    "        if i>15000: #setting number of tweets to scrape\n",
    "            break\n",
    "        tweets_list_21.append([tweet.date, tweet.id, \n",
    "                             tweet.content, tweet.user.username, \n",
    "                             tweet.coordinates]) #declare the attributes to be returned\n",
    "        \n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper\n",
    "                             (f'{keyword} lang:en since:2020-01-01 until:2020-12-31')\n",
    "                             .get_items()): #declaring the search word, language of tweet and \n",
    "                                            #the year 2020 for which tweets are extracted\n",
    "        if i>15000: #setting number of tweets to scrape\n",
    "            break\n",
    "        tweets_list_20.append([tweet.date, tweet.id, \n",
    "                             tweet.content, tweet.user.username, \n",
    "                             tweet.coordinates]) #declare the attributes to be returned\n",
    "    \n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper\n",
    "                             (f'{keyword} lang:en since:2019-01-01 until:2019-12-31')\n",
    "                             .get_items()): #declaring the search word, language of tweet and \n",
    "                                            #the year 2019 for which tweets are extracted\n",
    "        if i>15000: #setting number of tweets to scrape\n",
    "            break\n",
    "        tweets_list_19.append([tweet.date, tweet.id, \n",
    "                             tweet.content, tweet.user.username, \n",
    "                             tweet.coordinates]) #declare the attributes to be returned\n",
    "        \n",
    "# Creating a dataframe from the tweets list above \n",
    "tweets_df1 = pd.DataFrame(tweets_list_22, columns=['Datetime', 'Tweet Id', 'Text', 'Username','Location'])\n",
    "tweets_df2 = pd.DataFrame(tweets_list_21, columns=['Datetime', 'Tweet Id', 'Text', 'Username','Location'])\n",
    "tweets_df3 = pd.DataFrame(tweets_list_20, columns=['Datetime', 'Tweet Id', 'Text', 'Username','Location'])\n",
    "tweets_df4 = pd.DataFrame(tweets_list_19, columns=['Datetime', 'Tweet Id', 'Text', 'Username','Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ae989c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets= pd.concat([tweets_df1, tweets_df2, tweets_df3, tweets_df4], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6583ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Data frame to a csv file\n",
    "\n",
    "tweets.to_csv('vegan_years.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "407ed384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-07-19 23:58:23+00:00</td>\n",
       "      <td>1549544209450905600</td>\n",
       "      <td>Soup proof. The wife says it’s too much black ...</td>\n",
       "      <td>TechdommeWifey</td>\n",
       "      <td>Coordinates(longitude=-118.668404, latitude=33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-19 23:56:31+00:00</td>\n",
       "      <td>1549543739642765314</td>\n",
       "      <td>Nursing moms will appreciate this! Soothe pain...</td>\n",
       "      <td>crazyoilladies</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-07-19 23:55:52+00:00</td>\n",
       "      <td>1549543576953950209</td>\n",
       "      <td>Veganism should now be accepted as either a re...</td>\n",
       "      <td>chillangelino</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-19 23:55:23+00:00</td>\n",
       "      <td>1549543455218606080</td>\n",
       "      <td>Our signature White Musk® is the scent of a ge...</td>\n",
       "      <td>DelightsInspire</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-07-19 23:53:32+00:00</td>\n",
       "      <td>1549542989160009729</td>\n",
       "      <td>The most powerful Republican congressman from ...</td>\n",
       "      <td>TheGayVegans</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68131</th>\n",
       "      <td>17377</td>\n",
       "      <td>2019-01-01 10:53:33+00:00</td>\n",
       "      <td>1080054413161582592</td>\n",
       "      <td>1/2 New Year’s Wish: what if #plant #meetings ...</td>\n",
       "      <td>CG_ath</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68132</th>\n",
       "      <td>17378</td>\n",
       "      <td>2019-01-01 07:02:07+00:00</td>\n",
       "      <td>1079996174269104128</td>\n",
       "      <td>Best #plant-based foods: high in #protein: htt...</td>\n",
       "      <td>Seattle_Diners</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68133</th>\n",
       "      <td>17379</td>\n",
       "      <td>2019-01-01 07:01:20+00:00</td>\n",
       "      <td>1079995976050507777</td>\n",
       "      <td>Best #plant-based foods: high in #protein: htt...</td>\n",
       "      <td>OrganicLiveFood</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68134</th>\n",
       "      <td>17380</td>\n",
       "      <td>2019-01-01 01:48:36+00:00</td>\n",
       "      <td>1079917274507681792</td>\n",
       "      <td>1/6 #WFPB #Vegan #Carnivore #Keto #LCHF #Veget...</td>\n",
       "      <td>DrHorwitz</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68135</th>\n",
       "      <td>17381</td>\n",
       "      <td>2019-01-01 01:35:52+00:00</td>\n",
       "      <td>1079914070755561472</td>\n",
       "      <td>Jill Nussinow’s Smoky-Sweet Black-Eyed Peas an...</td>\n",
       "      <td>theveggiequeen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68136 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                   Datetime             Tweet Id  \\\n",
       "0               0  2022-07-19 23:58:23+00:00  1549544209450905600   \n",
       "1               1  2022-07-19 23:56:31+00:00  1549543739642765314   \n",
       "2               2  2022-07-19 23:55:52+00:00  1549543576953950209   \n",
       "3               3  2022-07-19 23:55:23+00:00  1549543455218606080   \n",
       "4               4  2022-07-19 23:53:32+00:00  1549542989160009729   \n",
       "...           ...                        ...                  ...   \n",
       "68131       17377  2019-01-01 10:53:33+00:00  1080054413161582592   \n",
       "68132       17378  2019-01-01 07:02:07+00:00  1079996174269104128   \n",
       "68133       17379  2019-01-01 07:01:20+00:00  1079995976050507777   \n",
       "68134       17380  2019-01-01 01:48:36+00:00  1079917274507681792   \n",
       "68135       17381  2019-01-01 01:35:52+00:00  1079914070755561472   \n",
       "\n",
       "                                                    Text         Username  \\\n",
       "0      Soup proof. The wife says it’s too much black ...   TechdommeWifey   \n",
       "1      Nursing moms will appreciate this! Soothe pain...   crazyoilladies   \n",
       "2      Veganism should now be accepted as either a re...    chillangelino   \n",
       "3      Our signature White Musk® is the scent of a ge...  DelightsInspire   \n",
       "4      The most powerful Republican congressman from ...     TheGayVegans   \n",
       "...                                                  ...              ...   \n",
       "68131  1/2 New Year’s Wish: what if #plant #meetings ...           CG_ath   \n",
       "68132  Best #plant-based foods: high in #protein: htt...   Seattle_Diners   \n",
       "68133  Best #plant-based foods: high in #protein: htt...  OrganicLiveFood   \n",
       "68134  1/6 #WFPB #Vegan #Carnivore #Keto #LCHF #Veget...        DrHorwitz   \n",
       "68135  Jill Nussinow’s Smoky-Sweet Black-Eyed Peas an...   theveggiequeen   \n",
       "\n",
       "                                                Location  \n",
       "0      Coordinates(longitude=-118.668404, latitude=33...  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3                                                    NaN  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "68131                                                NaN  \n",
       "68132                                                NaN  \n",
       "68133                                                NaN  \n",
       "68134                                                NaN  \n",
       "68135                                                NaN  \n",
       "\n",
       "[68136 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading csv file to analyse the vegan trend in twitter\n",
    "\n",
    "pd.read_csv('vegan_years.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef7e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
